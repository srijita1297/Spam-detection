{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Conv1D, Dropout, Dense, Input, Embedding, MaxPooling1D, Flatten, BatchNormalization, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import autograd\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(start):\n",
    "    now = time.time()\n",
    "    s = now - start\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS_IN_SEQ = 3000\n",
    "EMBED_DIM = 32\n",
    "MODEL_PATH = \"model/spam_detect_char\"\n",
    "ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43 unique tokens\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/dataset.pkl\", 'rb') as f:\n",
    "    sequences, labels, word2index = pickle.load(f)\n",
    "    \n",
    "num_words = len(word2index)\n",
    "print(f\"Found {num_words} unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sequence.pad_sequences(sequences, maxlen=MAX_WORDS_IN_SEQ, padding='post', truncating='post')\n",
    "targets = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (33716, 3000)\n",
      "Shape of label tensor: (33716, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', targets.shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, targets, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhoomilbsheta/deepl/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    }
   ],
   "source": [
    "input_seq = Input(shape=[MAX_WORDS_IN_SEQ, ], dtype='int32')\n",
    "embed_seq = Embedding(num_words + 1, EMBED_DIM, input_length=MAX_WORDS_IN_SEQ)(\n",
    "    input_seq)\n",
    "conv_1 = Conv1D(128, 5)(embed_seq)\n",
    "conv_1 = BatchNormalization()(conv_1)\n",
    "conv_1 = Activation(activation='relu')(conv_1)\n",
    "conv_1 = MaxPooling1D(pool_size=5)(conv_1)\n",
    "\n",
    "conv_2 = Conv1D(128, 5)(conv_1)\n",
    "conv_2 = BatchNormalization()(conv_2)\n",
    "conv_2 = Activation(activation='relu')(conv_2)\n",
    "conv_2 = MaxPooling1D(pool_size=5)(conv_2)\n",
    "\n",
    "conv_3 = Conv1D(128, 5)(conv_2)\n",
    "conv_3 = BatchNormalization()(conv_3)\n",
    "conv_3 = Activation(activation='relu')(conv_3)\n",
    "conv_3 = MaxPooling1D(pool_size=35)(conv_3)\n",
    "\n",
    "flat = Flatten()(conv_3)\n",
    "flat = Dropout(0.25)(flat)\n",
    "fc1 = Dense(128, activation='relu')(flat)\n",
    "dense_1 = Dropout(0.25)(flat)\n",
    "fc2 = Dense(2, activation='softmax')(fc1)\n",
    "\n",
    "model = Model(input_seq, fc2)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhoomilbsheta/deepl/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25287 samples, validate on 8429 samples\n",
      "Epoch 1/5\n",
      "25287/25287 [==============================] - 1085s - loss: 0.1626 - acc: 0.9380 - val_loss: 0.9829 - val_acc: 0.5105\n",
      "Epoch 2/5\n",
      "25287/25287 [==============================] - 1086s - loss: 0.1038 - acc: 0.9605 - val_loss: 0.2687 - val_acc: 0.8602\n",
      "Epoch 3/5\n",
      "25287/25287 [==============================] - 1083s - loss: 0.0777 - acc: 0.9713 - val_loss: 0.1434 - val_acc: 0.9470\n",
      "Epoch 4/5\n",
      "25287/25287 [==============================] - 1054s - loss: 0.0598 - acc: 0.9786 - val_loss: 0.9278 - val_acc: 0.7392\n",
      "Epoch 5/5\n",
      "25216/25287 [============================>.] - ETA: 2s - loss: 0.0395 - acc: 0.9864"
     ]
    }
   ],
   "source": [
    "model = load_model(MODEL_PATH)\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=5,\n",
    "    callbacks=[ModelCheckpoint(MODEL_PATH, save_best_only=True)],\n",
    "    validation_data=[x_test, y_test]\n",
    ")\n",
    "\n",
    "model.save(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MXNET Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MxModel(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MxModel, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.embed = gluon.nn.Embedding(input_dim=num_words + 1, output_dim=EMBED_DIM)\n",
    "            \n",
    "            self.conv1 = gluon.nn.Conv1D(channels=128, kernel_size=5)\n",
    "            self.conv2 = gluon.nn.Conv1D(channels=128, kernel_size=5)\n",
    "            self.conv3 = gluon.nn.Conv1D(channels=128, kernel_size=5)\n",
    "            \n",
    "            self.bnorm1 = gluon.nn.BatchNorm()\n",
    "            self.bnorm2 = gluon.nn.BatchNorm()\n",
    "            self.bnorm3 = gluon.nn.BatchNorm()\n",
    "            \n",
    "            self.fc1 = gluon.nn.Dense(units=128)\n",
    "            self.fc2 = gluon.nn.Dense(units=2)\n",
    "            \n",
    "            self.dropout = gluon.nn.Dropout(rate=0.25)\n",
    "    def hybrid_forward(self, F, x, *args, **kwargs):\n",
    "        x = self.embed(x)\n",
    "        x = F.relu(self.bnorm1(self.conv1(x)))\n",
    "        x = F.relu(self.bnorm2(self.conv2(x)))\n",
    "        x = F.relu(self.bnorm3(self.conv3(x)))\n",
    "        x = F.relu(self.dropout(self.fc1(x)))\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx_model = MxModel()\n",
    "mx_model.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "trainer = gluon.Trainer(mx_model.collect_params(), 'adam', {'learning_rate': 0.001})\n",
    "acc = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mx.io.NDArrayIter(data=x_train, label=y_train, batch_size=128, shuffle=True)\n",
    "test_data = mx.io.NDArrayIter(data=x_test, label=y_test, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    data_iterator.reset()\n",
    "    acc_test = mx.metric.Accuracy()\n",
    "    for batch in data_iterator:\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        acc_test.update(preds=output, labels=label)\n",
    "    return acc_test.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1--------------\n",
      "loss: 0.7064136266708374    acc:0.5\n",
      "loss: 0.6858754754066467    acc:0.5\n",
      "loss: 0.6862083077430725    acc:0.5000386757425742\n",
      "loss: 0.6491576433181763    acc:0.5009054221854304\n",
      "val acc: 0.5065104166666666\n",
      "14m 19s\n",
      "Epoch 2--------------\n",
      "loss: 0.6357454061508179    acc:0.50390625\n",
      "loss: 0.6397137641906738    acc:0.5003063725490197\n",
      "loss: 0.6163472533226013    acc:0.49176206683168316\n",
      "loss: 0.5043906569480896    acc:0.47775248344370863\n",
      "val acc: nan\n",
      "27m 20s\n",
      "27m 20s\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "smoothing_constant = .01\n",
    "mx_model.hybridize()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f\"Epoch {e+1}--------------\")\n",
    "    i = 0\n",
    "    train_data.reset()\n",
    "    for batch in train_data:\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = mx_model(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = mx.nd.mean(loss).asscalar()\n",
    "        acc.update(preds=output, labels=label)\n",
    "        if i % 50 == 0:\n",
    "            print(f\"loss: {curr_loss}    acc:{acc.get()[1]}\")\n",
    "        i += 1\n",
    "    print(f\"val acc: {evaluate_accuracy(test_data, mx_model)}\")\n",
    "    print(time_since(start))\n",
    "    acc.reset()\n",
    "    \n",
    "print(time_since(start))\n",
    "mx_model.save_params(\"data/mx_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
